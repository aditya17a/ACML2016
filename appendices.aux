\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\newlabel{jmlrstart}{{}{1}{}{Doc-Start}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Second Derivative Back-Propagation}{1}{section.0.A}}
\newlabel{apd:first}{{A}{1}{Second Derivative Back-Propagation}{section.0.A}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A computational graph of a simple feed-forward network illustrating the naming of different variables, where $\sigma (\cdot )$ is the nonlinearity, MSE is the mean-squared error cost function and $E$ is the overall loss.}}{1}{figure.1}}
\newlabel{fig:comp_graph}{{1}{1}{A computational graph of a simple feed-forward network illustrating the naming of different variables, where $\sigma (\cdot )$ is the nonlinearity, MSE is the mean-squared error cost function and $E$ is the overall loss}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}First and Second Derivatives}{2}{subsection.0.A.1}}
\newlabel{cost_func_derivative}{{2}{2}{First and Second Derivatives}{equation.0.A.2}{}}
\newlabel{cost_func_2nd_derivative}{{3}{2}{First and Second Derivatives}{equation.0.A.3}{}}
\newlabel{sigmoid_derivative}{{4}{2}{First and Second Derivatives}{equation.0.A.4}{}}
\newlabel{sigmoid_2nd_derivative}{{5}{2}{First and Second Derivatives}{equation.0.A.5}{}}
\newlabel{dedx}{{21}{3}{First and Second Derivatives}{equation.0.A.21}{}}
\newlabel{d2edx2}{{27}{3}{First and Second Derivatives}{equation.0.A.27}{}}
\newlabel{dxdc}{{30}{4}{First and Second Derivatives}{equation.0.A.30}{}}
\newlabel{dedc}{{32}{4}{First and Second Derivatives}{equation.0.A.32}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.1.1}Summary Of Output Layer Derivatives}{5}{subsubsection.0.A.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.1.2}Hidden Layer Derivatives}{5}{subsubsection.0.A.1.2}}
\newlabel{dcdo}{{49}{5}{Hidden Layer Derivatives}{equation.0.A.49}{}}
\newlabel{dedo_general}{{51}{6}{Hidden Layer Derivatives}{equation.0.A.51}{}}
\newlabel{d2edo2}{{61}{6}{Hidden Layer Derivatives}{equation.0.A.61}{}}
\newlabel{de2do2_general}{{62}{7}{Hidden Layer Derivatives}{equation.0.A.62}{}}
\newlabel{dedx_general}{{63}{7}{Hidden Layer Derivatives}{equation.0.A.63}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.1.3}Summary Of Hidden Layer Derivatives}{8}{subsubsection.0.A.1.3}}
\newlabel{jmlrend}{{A.1.3}{8}{end of The Incredible Shrinking Neural Network}{section*.1}{}}
